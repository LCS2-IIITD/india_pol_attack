{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJUnDFzgzC3q"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertPreTrainedModel, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, recall_score\n",
    "#ConfusionMatrixDisplay\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertPreTrainedModel, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "wcalmCCuTwcu",
    "outputId": "cff69fab-797a-40c3-f84b-df4289b77919"
   },
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '/device:GPU:0':\n",
    "    print(f'Found GPU at: {device_name}')\n",
    "device_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "GLZLsS3fUli7",
    "outputId": "02bb051f-81a4-4ee9-cdbf-456f602087ea"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU in use:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('using the CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aZUKWVkRBrQ"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128 # max sequences length\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IokttCXpR1_"
   },
   "outputs": [],
   "source": [
    "# extra preprocessing steps\n",
    "# prepend CLS and append SEP, truncate, pad\n",
    "\n",
    "# labels_encoding = {'NO':0,'Political Attack':1,'Politicians':1,'Taunt':2}\n",
    "# \n",
    "labels_encoding = {'N':0,'P':1,'O':1,'T':2}\n",
    "\n",
    "def preprocessing(df):\n",
    "    sentences = df.Text.values\n",
    "    ID = df['Unnamed: 0'].values\n",
    "    labels = np.array([labels_encoding[l] for l in df.Label.values])\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
    "    \n",
    "    encoded_sentences = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,\n",
    "                            add_special_tokens = True,\n",
    "                            truncation=True,\n",
    "                            max_length = MAX_LEN\n",
    "                    )\n",
    "        \n",
    "        encoded_sentences.append(encoded_sent)\n",
    "    encoded_sentences = pad_sequences(encoded_sentences, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                            value=0, truncating=\"post\", padding=\"post\")\n",
    "    return encoded_sentences, labels\n",
    "    \n",
    "def attention_masks(encoded_sentences):\n",
    "    # attention masks, 0 for padding, 1 for actual token\n",
    "    attention_masks = []\n",
    "    for sent in encoded_sentences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Sarah_Label1_Dup_Updated.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'New_Label':'Label'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['New_Label'].map({'N':0,'P':1,'O':1,'T':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZL4pCBWMH6E"
   },
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "def compute_accuracy(preds, labels):\n",
    "    p = np.argmax(preds, axis=1).flatten()\n",
    "    l = labels.flatten()\n",
    "    return np.sum(p==l)/len(l)\n",
    "\n",
    "def run_train(epochs,train_dataloader,validation_dataloader):\n",
    "\n",
    "#     print(\"hello\")\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        d = pd.DataFrame()\n",
    "        print('======== Epoch {:} / {:} ========'.format(e + 1, epochs))\n",
    "        start_train_time = time.time()\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            if step%10 == 0:\n",
    "                elapsed = time.time()-start_train_time\n",
    "                print(f'{step}/{len(train_dataloader)} --> Time elapsed {elapsed}')\n",
    "\n",
    "            # input_data, input_masks, input_labels = batch\n",
    "            input_data = batch[0]#.to(device)\n",
    "            input_masks = batch[1]#.to(device)\n",
    "            input_labels = batch[2]#.to(device)\n",
    "\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            # forward propagation\n",
    "            out = model(input_data,\n",
    "                        token_type_ids = None, \n",
    "                        attention_mask = input_masks,\n",
    "                        labels = input_labels)\n",
    "#             print(out)\n",
    "            \n",
    "            loss = out[0]\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss = total_loss/len(train_dataloader)\n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Training took {time.time()-start_train_time}\")\n",
    "\n",
    "        # Validation\n",
    "        start_validation_time = time.time()\n",
    "        model.eval()\n",
    "        eval_loss, eval_acc = 0,0\n",
    "        preds = []\n",
    "        evals = []\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            eval_data, eval_masks, eval_labels = batch\n",
    "            with torch.no_grad():\n",
    "                out = model(eval_data,\n",
    "                            token_type_ids = None, \n",
    "                            attention_mask=eval_masks)\n",
    "            logits = out[0]\n",
    "\n",
    "            #  Uncomment for GPU execution\n",
    "#             logits = logits.detach().cpu().numpy()\n",
    "#             eval_labels = eval_labels.to('cpu').numpy()\n",
    "#             batch_acc = compute_accuracy(logits, eval_labels)\n",
    "            p = np.argmax(logits, axis=1).flatten()            \n",
    "            preds.extend(p)\n",
    "            e = eval_labels.flatten()\n",
    "            evals.extend(e)\n",
    "\n",
    "\n",
    "            # Uncomment for CPU execution\n",
    "            # batch_acc = compute_accuracy(logits.numpy(), eval_labels.numpy())\n",
    "\n",
    "#             eval_acc += batch_acc\n",
    "#         print(f\"Accuracy: {eval_acc/(step+1)}, Time elapsed: {time.time()-start_validation_time}\")\n",
    "        print(f\"Time elapsed: {time.time()-start_validation_time}\")\n",
    "        d['Evals'] = evals\n",
    "        d['Preds'] = preds\n",
    "        print(classification_report(evals,preds))\n",
    "\n",
    "    return losses        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aAILAFFrTFhq",
    "outputId": "34ecefa6-28ee-4ecb-eedb-8196a8406a2a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "loss = []\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-multilingual-cased\",\n",
    "        num_labels = 3,   \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = False, )\n",
    "    \n",
    "for name,param in model.named_parameters():\n",
    "        if ((name.startswith(\"bert.encoder.layer.0.\")) or (name.startswith(\"bert.encoder.layer.1.\")) or \n",
    "        (name.startswith(\"bert.encoder.layer.2.\")) or (name.startswith(\"bert.encoder.layer.3.\")) or \n",
    "        (name.startswith(\"bert.encoder.layer.4.\")) or (name.startswith(\"bert.encoder.layer.5.\"))):\n",
    "            param.requires_grad = False\n",
    "            \n",
    "for train_index, test_index in kf.split(df):\n",
    "    \n",
    "    \n",
    "    train_encoded_sentences, train_labels = preprocessing(df.iloc[train_index])\n",
    "    train_attention_masks = attention_masks(train_encoded_sentences)\n",
    "\n",
    "    test_encoded_sentences, test_labels = preprocessing(df.iloc[test_index])\n",
    "    test_attention_masks = attention_masks(test_encoded_sentences)\n",
    "    \n",
    "    train_inputs = torch.tensor(train_encoded_sentences)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_masks = torch.tensor(train_attention_masks)\n",
    "\n",
    "    validation_inputs = torch.tensor(test_encoded_sentences)\n",
    "    validation_labels = torch.tensor(test_labels)\n",
    "    validation_masks = torch.tensor(test_attention_masks)\n",
    "    \n",
    "        # data loader for training\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = SequentialSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # data loader for validation\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    \n",
    "    import random\n",
    "\n",
    "    seed_val = 42\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "#     model.cuda()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = 3e-5, \n",
    "                      eps = 1e-8, \n",
    "                      weight_decay = 0.01\n",
    "                    )\n",
    "\n",
    "    epochs = 3\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # 10% * datasetSize/batchSize\n",
    "                                                num_training_steps = total_steps)\n",
    "\n",
    "    \n",
    "    losses = run_train(epochs,train_dataloader,validation_dataloader)\n",
    "#     loss.append(losses)\n",
    "# print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'mBERT')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "mBert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
